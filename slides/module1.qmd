---
title: Module 1
author: Matias Salibian Barrera and Daniel J. McDonald
format: 
  revealjs: default
  pdf: default
---

::: {.content-hidden}
{{< include _latexmacros.qmd >}}
:::

# Welcome & "House rules"

- Weird schedule (Mondays and Fridays)
- Many opportunities to work and get help
- Weekly ``routine'':
  - Class meetings: 3 hours
  - Pre-class reading: 1 to 2 hours
  - WebWork & assignments: 2 to 3 hours
  - Office hour
  - Total weekly hours: 7 to 9 hours
- Problem solutions are dangerous!!
- Online "solutions": you must
prove whatever you claim


# Syllabus - Expectations - Grades

# Basics of probability

- Formal treatment of randomness
- Key to this are models
- Defining a probability requires the following:
  - Random experiment
  - Sample space
  - Event
  - Rules to combine events (set operations)

## Experiments

### Definition 
An action undertaken to make a discovery,
test a hypothesis, or confirm a known fact.

### Example 
Release your pen from 4.9 meters above the ground.

### Predicted outcomes

- The pen will fall to the ground. 

- It will take about 1 sec to reach the ground.


## Actual observations

- The pen did touch the ground

- Less sure if it took exactly 1 second to do so



## Uncertainty

The outcome of some experiments cannot be determined
beforehand. 


::: Example
- Roll a die: which side will show? 

- Draw a card from a well-shuffled deck: which one you will get? 

- How many students will be in the classroom today?
:::


## Probability theory

- Even though die rolls are random, 
patterns emerge when we repeat the experiment many times.

- Probability Theory describes such patterns via mathematical
models.

- It is a branch of Mathematics, and is
based on a set of Axioms.

- Axioms: Statements or propositions accepted 
to hold true

- Theorems: Propositions which are 
established to hold true using sound logical
reasoning.

## Sample Space

::: {.Definition}
Sample space is the set of all possible outcomes of a
random experiment.
:::

We denote it by $\Omega$, and a generic outcome, also called sample
point, by $\omega$ (i.e. $\omega \in \Omega$).

::: Example
- Roll a die: $\Omega = \{ 1,2,3,4,5, 6 \} \subset \mathbb{N}$.

- Draw a card from a poker deck:
$\Omega = \{ 2\spadesuit, 2\diamondsuit, \ldots, A\clubsuit ,A \heartsuit  \}$
    
- Wind speed at YVR (km/h): $\Omega =[0, \infty) \subset \mathbb{R}$.

- Wait time for R4 at UBC (min): $\Omega =[0, 720) \subset \mathbb{R}$
:::


## Events

::: Definition
An event is a subset of the sample space $\Omega$.
:::

**Notation:** We commonly use upper case letters ($A$, $B$, $C$,
...) for events.

Events are sets:

- $\omega \in A$ means "$\omega$ is an element of $A$".

- $C \subset D$ means "$C$ is a subset of $D$".


## Examples

Events are often formed by outcomes sharing some
property. It's a good idea to practice listing explicitly 
the sample points of events described with words.

::: Example
- Roll a dice:
  - $A = \text{"roll an even number"} = \{ 2, 4, 6 \}$ \
  - $B = \text{"roll a 3 or less"} = \{1, 2, 3\}$
  - $C = \text{"roll an even number no higher than 3"} = \{ 2 \}$

- Bus wait time: $C = \text{"wait is less than half an hour"} = [15, 30]$

- Max-wind-speed: $A = \text{"wind is over 80 km/hour"} = (80, \infty )$
:::


## Set Operations

Suppose $A$, $B$ are events (subsets of $\Omega$).

- **Union:** $A \union B$ 
    $$\omega \in A \cup B \Leftrightarrow \omega \in A 
    \mbox{ or } \omega \in B$$ 

- **Intersection:** $A \cap B$ 
    $$\omega \in A \cap B \Leftrightarrow \omega \in A 
    \mbox{ and } \omega \in B$$ 

- **Complement:** $A^c$ 
    $$\omega \in A^c\Leftrightarrow \omega \notin A$$ 

- **Symmetric difference:** $A \, \triangle \, B$ 
    $$A \, \triangle \, B \, = \, \left( A \cap B^c \right) \, \cup \, \left( A^c \cap B \right)$$



## Properties of set operations

- Commutative: 

    - $A \cup B \ = \ B \cup A$

    - $A \cap B \ = \ B \cap A$

- Associative: 

    - $A\cup B\cup C \, = \, \left( A\cup B\right) \cup C=A\cup \left( B\cup C\right)$

    - $A\cap B\cap C \, = \, \left( A\cap B\right) \cap C=A\cap \left( B\cap C\right)$

    

- Distributive: 

    - $\left( A\cup B\right) \cap C \, = \, \left( A\cap C\right) \cup \left( B\cap
        C\right)$

    - $\left( A\cap B\right) \cup C \, = \, \left( A\cup C\right) \cap \left( B\cup
        C\right)$

    

- $A = B$ $\Leftrightarrow$ $A \subseteq B$ and $B \subseteq A$



## De Morgan's Laws

::: Theorem
For any two events (sets) $A$ and $B$, we have
$$
( A\cup B ) ^{c} \, = \, A^{c}\cap B^{c}
$$
:::


To prove the theorem it is sufficient 
to show that
$$
( A\cup B )^{c}  \subseteq  A^{c}\cap B^{c}
$$
and that 
$$
A^{c}\cap B^{c}  \subseteq  ( A\cup B ) ^{c}
$$



## Proof of De Morgan's Laws

::: Proof
Let us show $( A\cup B ) ^{c}  \subseteq  A^{c}\cap B^{c}$:

1. Take any $\omega \in  ( A\cup B )^c$, then we must have
    $\omega \notin A \cup B$;

2. This implies that $\omega \notin A$ and $\omega \notin B$ (because if
    either $\omega \in A$ or $\omega \in B$ then we'd have
    $\omega \in A \cup B$);

3. Hence: $\omega \in A^c$ and $\omega \in B^c$;

4. Which is the same as $\omega \in A^c \cap B^c$;

5. Thus, we showed that $\omega \in  ( A\cup B )^c$ implies
    $\omega \in A^c \cap B^c$. In other words, that 
    $$( A\cup B ) ^{c} \, \subseteq \, A^{c}\cap B^{c} \, .$$
    

We leave it to you to prove 
that $A^{c}\cap B^{c}  \subseteq  ( A\cup B ) ^{c}$.

:::


## Some Useful Identities



- $A \ =  ( A\cap B ) \, \cup \,  ( A\cap B^{c} )$ 

::: Proof
- First: $A \subseteq \left( A\cap B\right) \, \cup \, \left( A\cap B^{c}\right)$:
  1. Take $\omega \in A$, then either $\omega \in B$ or
        $\omega \notin B$. 
  2. In the first case: $\omega \in A \cap B$. In
        the second case: $\omega \in A \cap B^c$. 
  3. Thus $\omega \in \left( A\cap B\right) \, \cup \, \left( A\cap B^{c}\right)$.
- Also: $\left( A\cap B\right) \, \cup \, \left( A\cap B^{c}\right) \subseteq A$:
  1. Take $\omega \in \left( A\cap B\right) \, \cup \, \left( A\cap B^{c}\right)$.
  2. If $\omega \in \left( A\cap B\right)$ then $\omega \in A$, and if 
  $\omega \in \left( A\cap B^{c}\right)$ then $\omega \in A$.
  4. Thus, we always have $\omega \in A$.
:::
 

- $A \, \cup\,  B \ = \ A \, \cup \left( B\cap A^{c}\right)$
  - Prove it!



## Power set

The power set of $\Omega$ (denoted $2^\Omega$) is the set of all
possible subsets of $\Omega$. 

For example, if 
$\Omega \ = \ \{ 1,2,3 \}$ then:
$$2^{\Omega }  \, 
=  \Bigl\{ 
\varnothing ,
\{ 1\} , \{ 2 \} , \{ 3 \} , \{ 1,2 \} , \{ 1,3 \} , \{ 2,3 \}, \{ 1, 2, 3 \} \Bigr\}$$

- The symbol $\varnothing$ denotes the empty set. 

- If $\Omega$ has $n$ elements, then $2^{\Omega }$ has $2^{n}$
    elements. In symbols: $$\#  ( 2^{\Omega } ) \ = \ 2^{\#\Omega }$$
    How can we prove this when $\#\Omega = n \in \mathbb{N}$? 


## Counting / listing

List the $n$ elements of $\Omega$ as
$\Omega =\left\{ \omega _{1},\omega _{2},...,\omega _{n}\right\}$.

Any event $A \subseteq \Omega$ can be uniquely represented by a 
sequence of 0's and 1's:
$$
x_i = \left\{ \begin{array}{ll} 1 & \text{ if } \omega_i \in A \, , \\
0 & \text{ if } \omega_i \notin A \, . \end{array} \right. 
\quad 1 \le i \le n \, .
$$
To each event $A$ we associate the string $(x_1, x_2, \ldots, x_n)$:
$$
A \ \longrightarrow \ (x_1, x_2, \ldots, x_n)
$$
And, each sequence corresponds to
an event: $(0, 1, 1, \ldots, 0, 1) \ \longrightarrow \ B$.

There are $2^n$ distinct such $n$-long sequences. 
Each of
them corresponds uniquely to one event, and every event 
corresponds to one such sequence. Hence there are the same
number of events as there are sequences ($2^n$). 

## Counting / listing

::: Example
How many license plates can you create using a triplet of
one letter, one digit, and one letter (e.g. R2A)?
:::

# Probability 

## Probability of an event

- Even though random outcomes cannot be predicted, 
  in some cases we have an idea about the chance that
  an outcome occurs. 

  - If you toss a fair coin, the chance of observing a head is 
    the same as that of observing a tail.

  - If you buy a lottery ticket, the chance of winning is very small.

- A probability function $\P$ quantifies these chances.
  
- Probability functions are computed on events $A \in \mathcal{B}$. 
  We calculate $\P(A)$. Mathematically / formally, we have:
  $$
  \P \, : \, \mathcal{B} \to [0, 1] 
  $$
  where $\mathcal{B}$ is a collection of possible events. 

- Probability functions need to do this "coherently"

## Probability Axioms

Let $\Omega$ be a sample space and ${\cal B}$ be a collection of 
events (i.e. subsets of $\Omega$). 

::: Definition
A probability function is a function $\P$ with domain ${\cal B}$ such that

1.  **Axiom 1: ** $\P ( \Omega ) = 1$;

2.  **Axiom 2: ** $\P ( A ) \geq 0$ for any $A \in {\cal B}$; 
    
3.  **Axiom 3: ** If $\{ A_{n}\}_{n \ge 1}$ is a sequence of
    **disjoint** events, then
    $$\P\left( \bigcup_{n=1}^{\infty }A_{n}\right) \, 
    = \sum_{n=1}^{\infty }\P ( A_{n})$$
:::

Note: $\{ A_{n}\}_{n \ge 1}$ is a sequence of
    **disjoint** events when $A_i \cap A_j = \varnothing$ if $i \ne j$


## Probability

- Kolmogorov showed how one can construct such functions,
and that a probability function only needs to
satisfy those three properties to 
be a "coherent" probability function

- In other words: every desirable property of a
probability $\P$ can be shown to hold using 
only Axioms 1, 2, and 3 (and logic). 

- Alternatively: any function $\P$ that satisfies
Axioms 1, 2, and 3 is a "proper" probability function. 


## Properties of the probability function

In general, $A$, $B$, $C$, etc. denote 
arbitrary events. $\Omega$ is the sample space.



- Probability of the complement: $$\P ( A^{c} ) =1-\P ( A )$$

- Monotonicity: $$A\subset B\Rightarrow \P ( A ) \leq \P (B )$$

- Probability of the union:
    $$\P ( A\cup B ) =\P ( A ) +\P ( B ) - \P ( A\cap B )$$

- Boole's inequality:
    $$\P ( \bigcup _{i=1}^{m}A_{i} ) \leq \sum_{i=1}^{m}\P ( A_{i} )$$






## Proof of formula for complement

We need to show that if $\P$ satisfies Axioms 1, 2, and 3, and 
$A$ is an arbitrary event, then 
necessarily $\P ( A^{c} ) \ = \ 1-\P ( A )$. 

::: Proof
### 
1. $\Omega = A \cup A^{c}$ and $A$ and $A^c$ are
    disjoint.
    
2. Using Axiom 1
   $$ 1 = \P(\Omega) = \P( A \cup A^{c} )$$

3. Axiom 3 says that $\P( A \cup A^{c} ) = \P (A  ) +\P ( A^c)$

4. Putting the last 2 statements together we get $1 =  \P ( \Omega )  =  \P (A  ) +\P ( A^c)$.

    Since $\P(A)$ and $\P(A^c)$ are just numbers, simple arithmetic gives 
    $$1-\P ( A ) = \P ( A^{c} )$$
:::



## Monotonicity

::: Proof
### $A \subset B \, \Rightarrow \, \P ( A) \leq \P ( B)$



- We note that $B =  ( B\cap A ) \cup  ( B\cap A^{c})$; 

- Since $A \subset B$ is given, we have $B \cap A = A$. Hence,
    $B =  A \cup ( B \cap A^{c})$ 

- In addition, $A$ and $B \cap A^{c}$ are disjoint. By Axiom 3 , we
    get $\P ( B )  =  \P ( A ) + \P ( B\cap A^{c})$.

    

- By Axiom 2, $\P ( B\cap A^{c}) \ge 0$. Thus,
    $$\P ( B ) = \P ( A ) + P  ( B\cap A^{c}) \ge  \P ( A ).$$ 
    
:::



## Probability of a union

::: Proof
### $\P ( A\cup B ) =\P ( A ) +\P( B ) -\P ( A\cap B )$

- First recall that $A\cup B = A\cup  ( B\cap A^{c} )$.

- Note that $A$ and $B\cap A^{c}$ are disjoint events. Hence by Axiom
    3: $$\P ( A\cup B ) =\P ( A ) +\P ( B \cap A^{c} )$$

- Splitting $B$ into union of two disjoint events:
    $B  =  ( B\cap A ) \cup  ( B\cap A^c )$ and applying Axiom 3, we get
    $$\P ( B) = \P ( B\cap A ) +\P ( B\cap A^c )$$

- The formula is obtained by combining the above two conclusions.
:::


## Proof of Boole's inequality

$$\P\left( \bigcup _{i=1}^{n}A_{i}\right)  \leq \sum_{i=1}^{n}\P\left( A_{i}\right)$$

::: Proof

- We will prove it by induction.

- For $n=2$: $$\begin{aligned}
    \P\left( A_{1}\cup A_{2}\right) & = \P\left( A_{1}\right) +\P\left( A_{2}\right)
    -\P\left( A_{1}\cap A_{2}\right) \\
    \\
    & \leq \P\left( A_{1}\right) +\P\left(
    A_{2}\right)\end{aligned}$$ because
    $\P\left( A_{1}\cap A_{2}\right) \geq 0$

:::

## Boole's inequality (continued)

::: Proof

- Assume now that it holds for $n$. We then have 
$$
\begin{aligned}
\P \Big ( \bigcup _{i=1}^{n+1}A_{i} \Big ) 
& = 
\P\Big [ \Big ( \bigcup_{i=1}^{n}A_{i}\Big ) \bigcup A_{n+1}\Big ] &\mbox{ associative prop.} \\
& \leq 
\P\Big[ \bigcup_{i=1}^{n}A_{i}\Big ] +\P \left( A_{n+1} \right) &\mbox{ case n=2} \\
& \leq
\sum_{i=1}^{n} \P\left( A_{i}\right) +\P\left( A_{n+1} \right) &\mbox{ induction} \\
& = 
\sum_{i=1}^{n+1} \P\left( A_{i}\right).
\end{aligned}
$$

:::

## Example of applying these formulas


John borrows 2 books.

Suppose there is a 0.5 probability he likes the first book, 0.4
that he likes the second book, and 0.3 that he likes both.

What is the probability that he will **NOT** like either of the 2
books?



## Solution

::: {.content-hidden when-format="revealjs"}

Introduce two notations for events:
$$A  = \{ \mbox{ John likes book 1 }\}, 
  B = \{ \mbox{ John likes book 2 }\}.$$

We are told that
$\P ( A )  =  0.5 \, , \P ( B ) = 0.4 \, ,  \P ( A\cap B ) =0.3$.

We are asked to calculate $\P\left( A^{c}\cap B^{c}\right)$.


Note that $A^{c}\cap B^{c} =  ( A\cup B ) ^{c}$. Hence,
$$\P\left( A^{c}\cap B^{c}\right)  
  = \P\left[ \left( A\cup B\right) ^{c}\right]
  =1 - \P\left( A\cup B\right).$$

Making use of another formula:
$$\P\left( A\cup B\right) = \P\left( A\right) +\P\left( B\right) -\P\left( A\cap B\right) = 0.6.$$

We get $\P( A^{c}\cap B^{c})  = 1 - 0.6 = 0.4.$

**General line of approach**:

Connect $A^{c}\cap B^{c}$ with events $A$, $B$ and $A\cap B$, because
the probabilities of latter are provided.

:::


## Example of applying formulas

Jane must take two tests, call them $T_1$ and $T_2$.

Suppose the probability she passes test $T_1$ is **0.8**, test
$T_2$ is **0.7** and both tests is **0.6**.

Calculate the probability that:

(a) She passes at least one test. 

(b) She passes at most one test. 

(c) She fails both tests. 

(d) She passes only one test.



## Solutions

::: {.content-hidden when-format="revealjs"}

Notation: $$\begin{aligned}
A &= \{ \text{Jane passes test $T_1$} \} \\
B &= \{ \text{Jane passes test $T_2$} \}\end{aligned}$$

We are told that $$\P\left( A\right) =0.80, 
\P\left( B\right) =0.70, \text{  \ and
\ }\P\left( A\cap B\right) =0.6.$$

Our task is to use these information to figure out the probabilities of
various events.

:::

## Solution to (a) Jane passes at least one test

::: {.content-hidden when-format="revealjs"}

We note that
$$\left\{ \mbox{Passes at least one test}\right\} = A \cup B.$$

Hence, $$\begin{aligned}
\P\left( A\cup B\right) 
& = 
\P\left( A\right) +\P\left( B\right) -\P\left( A\cap B\right) \\
&=
0.80+0.70-0.60 \\ 
& =  0.90.
\end{aligned}$$

The probability that Jane passes at least 1 test is 0.9.

:::

## Solution to (b) Jane passes at most one test

::: {.content-hidden when-format="revealjs"}

We notice that $$\begin{aligned}
\{ \text{Passes at most one test}\} 
&=
\left\{ \text{Fails at least one test}\right\} \\
&=
A^{c}\cup B^{c}.\end{aligned}$$

And we notice
$$A^{c}\cup B^{c} = (A\cap B) ^{c}\text{ \ \ De Morgan rule\ \ }$$ which
gives one way of computing its probability.


$$\begin{aligned}
\P ( \{ \text{Passes at most one test} \}  )
&= \P\left(\left( A\cap B\right) ^{c}\right) \text{ \ \ \ } \\
&=1-\P\left( A\cap B\right) \\
&=1-0.60=0.40.\end{aligned}$$ 

The probability that Jane passes at most 1 test is 0.4.

:::



## Solution to (c) She fails both tests

::: {.content-hidden when-format="revealjs"}

Similarly, we observe by De Morgan rule, $$\begin{aligned}
\{ \text{Fails both tests}\} 
&=
A^{c}\cap B^{c} \\
&=
( A\cup B) ^{c} \end{aligned}$$ which gives one way of computing its
probability.

$$\begin{aligned}
\P (  \{ \text{Fails both tests} \}  )
 &=
 \P\left( \left( A\cup B\right) ^{c}\right) \\
&=1-\P\left( A\cup B\right) \\
&=1-0.90 \\
&=
0.10 \text{  from Part (a)}.\end{aligned}$$ 

Therefore, the probability that Jane fails both tests is 0.1.
 
:::


## Solution to (d) She passes only one test

::: {.content-hidden when-format="revealjs"}

We first decompose the event into the union of two disjoint events:
$$\left\{ \text{Passes only one test}\right\} =%
\left( A\cap B^{c}\right) 
\cup 
\left( A^{c}\cap B\right)$$

We hence have 
$$
\begin{aligned}
\P(\left\{ \text{Passes only one test}\right\})
&=
\P\left( A\cap B^{c}\right) 
+
\P\left( A^{c}\cap B\right)  \\
&= 
\left[ \P\left( A\right) -\P\left( A\cap B\right) \right]
+ \left[ \P\left( B\right) -\P\left( A\cap B\right) \right] \\
&=
\P\left( A\right) +\P\left( B\right) -2 \times \P\left( A\cap B\right) \\
&=
0.80+0.70-2\times 0.60  = 
0.30.
\end{aligned}
$$ 

The probability that Jane passes only 1 test is 0.3.

:::



## Remarks

- It is typical in introductory probability theory course to give a
    story first, followed by specifying events verbally.
    
- In these cases, your answer should be a sentence, not just with
  the context included.

- The best mathematical approach is to **define** some events, and
    express other events of interest using those events.
    
- Rather than relying on algebra as in our examples, it can be easier
    to use Venn diagram to have these events connected.

- There can be many ways to connect them, all lead to a viable
    probability calculation.

## Exercise without a story

Exercise 2

(a) Suppose that $\P\left( A\right) =0.85$ and $\P\left( B\right) =0.75.$
    Show that $$\P\left( A\cap B\right) \geq 0.60.$$ 

(b) More generally, prove the Bonferroni inequality:
    $$\P\left( \cap _{i=1}^{n}A_{i}\right) \geq \sum_{i=1}^{n}\P\left( A_{i}\right)
    -\left( n-1\right) .$$



## Solution to 2 (a)

::: {.content-hidden when-format="revealjs"}

We know that $\P\left( A\right) =0.85$ and $\P\left(
B\right) =0.75$ and wish to show that
$$\P\left( A\cap B\right) \geq 0.60.$$

Note that $A\cap B=\left( A^{c}\cup B^{c}\right) ^{c}$ (De Morgan
rule)

Hence:
$$\P\left( A\cap B\right) = 1-\P\left( A^{c}\cup B^{c}\right)$$

Also
$$\P\left( A^{c}\cup B^{c}\right)  \leq \P\left( A^{c}\right) +\P\left( B^{c}\right)$$


Therefore $$\begin{aligned}
\P\left( A\cap B\right) 
&=
\P\left( \left( A^{c}\cup B^{c}\right) ^{c}\right) \\
&=
1-\P\left( A^{c}\cup B^{c}\right)  \\
&\geq 
1-\left[ \P\left( A^{c}\right) +\P\left( B^{c}\right) \right]  \\
&=
1-\left[ 1-\P\left( A\right) \right] -\left[ 1-\P\left( B\right) \right]  \\
&=
\P\left( A\right) +\P\left( B\right) -1 \\
& =
0.85+0.75-1=0.60.\end{aligned}$$

:::



## Solution to Example 2 (b) Prove the Bonferroni inequality

::: {.content-hidden when-format="revealjs"}

$$\begin{aligned}
\P\left( \bigcap _{i=1}^{n}A_{i}\right) &\geq\sum_{i=1}^{n}\P\left(
A_{i}\right) -\left( n-1\right) . \\
&&\end{aligned}$$

First note that $$\bigcap _{i=1}^{n}A_{i} \, = \,  
\left( \bigcup _{i=1}^{n}A_{i}^{c}\right) ^{c}.$$


Therefore 
$$
\begin{aligned}
\P\left( \bigcap _{i=1}^{n}A_{i}\right) & =\P\left( \left( \bigcup
_{i=1}^{n}A_{i}^{c}\right) ^{c}\right) \\
& =1-\P\left( \bigcup _{i=1}^{n}A_{i}^{c}\right)  \\
& \geq 1-\sum_{i=1}^{n}\P\left( A_{i}^{c}\right) \\
& =1-\sum_{i=1}^{n}\left( 1-\P\left( A_{i}\right) \right) &\text{ \ complement
rule} \\
& =1-\sum_{i=1}^{n}1+\sum_{i=1}^{n}\P\left( A_{i}\right)
=1-n+\sum_{i=1}^{n}\P\left( A_{i}\right) \\
& =\sum_{i=1}^{n}\P\left( A_{i}\right) -\left( n-1\right) .
\end{aligned}$$

:::



## Refresh your memory of the probability axioms

Kolmogorov's axioms tell us that if we have (a) sample space, (b)
collection of events, and wish to create a probability function
$\P(\cdot)$; what properties this function should have.

The last a few examples show that if such a function $\P(\cdot)$ has
been given, how one may derive the value of $\P(A)$ from the
probabilities of other events.

We next suggest a way to set up (a) sample space, (b) collection of
events, and (c) probability function $\P(\cdot)$ for a special type of
experiments.

# Building probabilitie functions

## Equally likely outcomes

Suppose the sample space $\Omega$ is finite.
$$\Omega \, = \, \Bigl\{ \omega _{1},\omega _{2}, \ldots,\omega _{n}\Bigr\}$$

In many applications, we trust that these distinct outcomes are equally
likely so that we wish to make
$$\P (   \{ \omega _{i} \} )  = a \, , \quad a \in (0, 1].$$

For such an experiment, we get a natural probability function.



## Equally likely outcomes

Notice that $\Omega = \bigcup _{i=1}^{n} \left\{ \omega _{i}\right\}$,
and $\{\omega _{i}\}_{i=1}^n$ are disjoint events.

The probability theory Axioms require 
$$\begin{aligned}
1 &= \P\left( \Omega \right)= \P \Big ( \bigcup _{i=1}^{n}\left\{ \omega_{i}\right\} \Big ) 
= \sum_{i=1}^{n}\P\left( \left\{ \omega _{i}\right\} \right) 
=\sum_{i=1}^{n} a \, = \, n a.
\end{aligned}$$

Our only option is to let $a = 1 / n$.



## Equally likely outcomes

The axioms further require that for any event $A \subseteq \Omega$:
$$\begin{aligned}
\P(A)
&= \sum_{\omega _{i}\in A} \P( \{ \omega_{i} \})
= \sum_{\omega _{i}\in A}\frac{1}{n} =
\frac{\#A}{\#\Omega}\\
&=
\frac{\#\{\mbox{favorable outcomes} \}}{\#\{\mbox{possible outcomes}\}}
\end{aligned}$$

Being "favourable" means those in the event of interest: $A$ in
this example.



## Probability calculation examples



- To calculate probabilities, we count the number of sample points in
    the event of interest.

    

- Counting the number of sample points in a set can be mathematically
    surprisingly complicated.

    

- **Combinatorial theory** deals with this problem.

    

- We will learn some basic combinatorial rules and techniques for this
    purpose.



## Counting Example I

A die is rolled repeatedly until we see an outcome being 6.

(a) Specify/describe the sample space.

(b) Let $E_{n}$ denote the event that the number of rolls is exactly $n$
    ($n=1,2, \ldots$). Describe the event $E_{n}$.

(c) Describe the event $E_{1}\cup E_{2}$ and
    $( \bigcup _{n=1}^{\infty} E_{n}) ^{c}$.



## Solution to (a) Describe the sample space

::: {.content-hidden when-format="revealjs"}

The sample space consists of all sequences
$$\left( x_{1},x_{2}, \ldots, x_{n-1}, x_{n}\right)$$ with
$$1 \leq x_{i}\leq 5 \text{\ \ for \ \ } 1 \le i \le n-1 \text{, \ \ and \ } x_{n} = 6 .$$

For example: $$\begin{aligned}
\left( 4,1,5,1,6\right) \ \ \ \text{with }n &=&5 \\
\left( 5,2,6\right) \ \ \ \text{with }n &=&3 \\
\left( 6\right) \ \ \ \text{with }n &=&1 \\
\left( 2,4,5,1,3,4,6\right) \ \ \ \text{with }n &=&7\end{aligned}$$

:::

## Solution to (b) Describe the event...rolls is $n$

::: {.content-hidden when-format="revealjs"}

$$\begin{aligned}
E_{1} &=\left\{ (6)\right\} \\
E_{2} &=\left\{ \left( x_{1},6\right) :1\leq x_{1}\leq 5\right\} \\
E_{3} &=\left\{ \left( x_{1},x_{2},6\right) :1\leq x_{1},x_{2}\leq 5\right\}\\
&\;\; \vdots \\
E_{n} &=\left\{ \left( x_{1},x_{2},...,x_{n-1},6\right) :1\leq
x_{1},x_{2},...,x_{n-1}\leq 5\right\}\end{aligned}$$

:::

## Solution (c) $E_{1}\cup E_{2}$ and $( \bigcup_{n=1}^{\infty} E_{n}) ^{c}$

Verbally interpreting
$E_{1}\cup E_{2}$, $\bigcup_{n=1}^{\infty} E_{n}$, and
$( \bigcup_{n=1}^{\infty } E_n) ^{c}$:

::: {.content-hidden when-format="revealjs"}

Suggestive answers: 
$$\begin{aligned}
E_{1}\cup E_{2} &= \Bigl\{ \text{6 appears before the 3}^{rd}\text{ roll} \Bigr\} \\
\bigcup _{n=1}^{\infty }E_{n} & =  \Bigl\{ \mbox{6 \ eventually appears}\Bigr\}\\
\ \left( \bigcup _{n=1}^{\infty }E_{n}\right) ^{c} &=\Bigl\{ \mbox{6 never
appears}\Bigr\}
\end{aligned}$$

:::

## Counting example II

A system has **5 components**, which can either be **working** or
have **failed**.

The experiment consists of observing the current status (W/F) of
the 5 components.

(a) Describe the sample space for this experiment.

(b) How much is $\# \Omega$?

(c) Suppose that the system will work if either components (1 and 2), or
    (3 and 4), or (1, 3 and 5) are working.

    List the outcomes in the event
    $D =\left\{ \mbox{The system works}\right\}$?

(d) Let $A= \{ \text{components 4 and 5 have failed} \}$. How much is
    $\# A$?

(e) List the outcomes in $A \cap D$.



## Answer to (a) Describe the sample space

::: {.content-hidden when-format="revealjs"}

The outcomes of the experiment are sequences
$$( x_{1},x_{2}.x_{3},x_{4},x_{5} )$$ where we could have $x_i = W$ or
$x_i = F$ depending on when component $i$ is working or has failed.

One typical outcome is (W, W, F, W, F) which indicates that components
**1**, **2** and **4** are working and **3** and **5** have failed,

:::

## Answer to (b) How many outcomes are there in $\Omega$?

NOTE: the
number of elements of a set $A$ (aka its cardinal number) is denoted by
 $\#A.$ or $|A|$ or (sometimes) $\Vert A \Vert$

::: {.content-hidden when-format="revealjs"}

We have $\# \Omega = 2^{5} = 32$.

:::



## Answer to (c) When does it work?

**(c) The system works if (1 and 2), or (3 and 4) or (1, 3 and 5)
work.**

Let $D = \{ \mbox{The system works}\}$. Let us count it.

::: {.content-hidden when-format="revealjs"}

Here is an exhaustive list of $D$:

| 1 and 2 work      | 3 and 4 work     | 1, 3 and 5 work  |
|------------------|------------------|------------------|
| (W, W, W, W, W)   | (F, F, W, W, W)  | (W, F, W, F, W)  |
| (W, W, F, W, W)   | (F, W, W, W, W)  |                  |
| (W, W, W, F, W)   | (W, F, W, W, W)  |                  |
| (W, W, W, W, F)   |                  |                  |
| (W, W, W, F, F)   | (F, F, W, W, F)  |                  |
| (W, W, F, W, F)   | (F, W, W, W, F)  |                  |
| (W, W, F, F, W)   | (W, F, W, W, F)  |                  |
| (W, W, F, F, F)   |                  |                  |


Apparently, $\# D = 15$.

:::


## Answer to (d) How big is $|A|$?

**(d) A**$=\left\{ \text{\textbf{4 and 5 have failed}}\right\}$. How
many outcomes are there in A?

::: {.content-hidden when-format="revealjs"}

The outcomes in $A$ are of the form $(x_{1},x_{2},x_{3}, F, F)$ with
$x_{i}=W$ or $x_i = F$ for $i = 1, 2, 3$.

Hence, we have $2^{3}=8$ different outcomes: $$\# A = 2^{3} = 8 \, .$$

:::

## Answer to (e) Describe all the outcomes in $A \cap D$

::: {.content-hidden when-format="revealjs"}

Recall the list of $D$: 

 | 1 and 2 work        | 3 and 4 work     | 1, 3 and 5 work  |
|---------------------|------------------|------------------|
| (W, W, W, W, W)      | (F, F, W, W, W)  | (W, F, W, F, W)  |
| (W, W, F, W, W)      | (F, W, W, W, W)  |                  |
| (W, W, W, F, W)      | (W, F, W, W, W)  |                  |
| (W, W, W, W, F)      |                  |                  |
| **(W, W, W, F, F)**  | (F, F, W, W, F)  |                  |
| (W, W, F, W, F)      | (F, W, W, W, F)  |                  |
| (W, W, F, F, W)      | (W, F, W, W, F)  |                  |
| **(W, W, F, F, F)**  |                  |                  |

Clearly, we have
$$W\cap A= \{ ( W, W, W, F, F ) , ( W, W, F, F, F )  \}$$ and
$\#(W\cap A) = 2$.

:::


## Example III

Two dice have two sides painted red, two painted black, one painted
yellow, and the other painted white.

When this pair of dice is rolled, what is the probability that both
dice show the same color facing up?

**Remark**: If not explicitly declared, this type of problem
assumes "equal likely outcomes".



## Answer to Example III

::: {.content-hidden when-format="revealjs"}

We answer this question with brute-force counting.

An exhaustive list of sample space is presented as follows:

|     | R1 | R2 | B3 | B4 | Y5 | W6 |
|-----|----|----|----|----|----|----|
| R1  | X  | X  |    |    |    |    |
| R2  | X  | X  |    |    |    |    |
| B3  |    |    | X  | X  |    |    |
| B4  |    |    | X  | X  |    |    |
| Y5  |    |    |    |    | X  |    |
| W6  |    |    |    |    |    | X  |


The size of sample space is $\# \Omega = 36$.

The number of favourable outcomes is $10$.

Hence, under the equal probability model:
$$\P \{ \text{Same color} \} = {10}/{36} = {5}/{18}.$$

:::

## Example IV

A small community consists of 20 families.

Four of them have 1 child, 8 have 2 children, 5 have 3 children, 2
have 4 children, and 1 has 5 children.

(a) What is the probability that a randomly chosen family has $i$
    children, for each $1 \le i \le 5$? 

(b) What is the probability that a randomly chosen child comes from a
    family with $i$ children, for each $1 \le i \le 5$?

**Jargon**: By "randomly chosen", it says that the **specific
unit** is equally likely selected.

Applying the equal probability experiment/model for (a): family; (b):
child.



## Answer to Example IV

We organize the information as follows:

| i | Families with i children | Children from families w/ i children |
|---|---------------------------|--------------------------------------|
| 1 | 4                         | 4                                    |
| 2 | 8                         | 16                                   |
| 3 | 5                         | 15                                   |
| 4 | 2                         | 8                                    |
| 5 | 1                         | 5                                    |
| **Total** | **20**           | **48**                               |


**Catch**: There are 20 families, 48 children in this community.


## Answer to (a)

If one family is chosen at random, what is the probability
it has $i$ children, $i=1, 2, 3, 4, 5$?

::: {.content-hidden when-format="revealjs"}

| i | Families with i children | $\P$(family has i children) |
|---|---------------------------|---------------------------|
| 1 | 4                         | 4/20                      |
| 2 | 8                         | 8/20                      |
| 3 | 5                         | 5/20                      |
| 4 | 2                         | 2/20                      |
| 5 | 1                         | 1/20                      |
| **Total** | **20**           | **1.00**                  |

:::

## Answer to (b)

If one child is randomly chosen, what is the probability that it
comes from a family having $i$ children, $i=1,2,3,4,5$?

::: {.content-hidden when-format="revealjs"}

| i | Children from families with i children | $\P$(child comes from family with i children) |
|---|----------------------------------------|--------------------------------------------|
| 1 | 4                                      | 4/48                                       |
| 2 | 16                                     | 16/48                                      |
| 3 | 15                                     | 15/48                                      |
| 4 | 8                                      | 8/48                                       |
| 5 | 5                                      | 5/48                                       |
| **Total** | **48**                         | **1.00**                                   |

:::

**Remark**
Part (a) and Part (b) are probability calculations under **Two
Different** experiments.

